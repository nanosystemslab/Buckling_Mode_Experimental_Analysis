#!/usr/bin/env python3
"""
Buckling Mode Experimental Analysis Tool

A tool for extracting maximum force values from CSV files generated by 
experimental buckling tests.
"""

import pandas as pd
import os
import types
import logging
import argparse
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional

__version__ = "0.1.0"

def setup_logging(level: str = "INFO") -> None:
    """Set up logging configuration."""
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f'Invalid log level: {level}')
    
    logging.basicConfig(
        level=numeric_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

def load_file_TT(filepath: str) -> pd.DataFrame:
    """
    Load a single CSV file using the standard loading pattern.
    
    Args:
        filepath: Path to the CSV file
        
    Returns:
        DataFrame with Force and Stroke columns and metadata
    """
    log = logging.getLogger(__name__)
    log.debug(f"Loading file: {filepath}")
    
    if not ".csv" in filepath:
        raise ValueError("File must be a CSV file")
    
    # Load CSV with standard format (skip metadata rows)
    df = pd.read_csv(filepath, skiprows=1)
    df.drop(0, axis=0, inplace=True)
    
    # Convert to proper data types
    df["Force"] = df["Force"].astype(float)
    df["Stroke"] = df["Stroke"].astype(float)
    
    # Parse filename to extract metadata
    fname = os.path.basename(filepath)
    slugs = Path(fname).stem
    
    # Split on underscore to parse filename components
    parts = slugs.split("_")
    
    can_height = parts[0]           # [0] is the can height (e.g., "90mm")
    run_num = parts[-1]             # [-1] is the run number (e.g., "05")
    
    # Everything else between [0] and [-1] are notes about the sample
    notes = ""
    if len(parts) > 2:
        notes = "_".join(parts[1:-1])  # Join middle parts back with underscores
    
    # Add metadata namespace
    df.meta = types.SimpleNamespace()
    df.meta.filepath = filepath
    df.meta.can_height = can_height
    df.meta.test_run = run_num
    df.meta.notes = notes
    
    log.debug(f"Loaded {len(df)} data points from {fname}")
    return df

def extract_max_force_from_df(df: pd.DataFrame) -> float:
    """
    Extract maximum force from a loaded DataFrame.
    
    Args:
        df: DataFrame with Force column
        
    Returns:
        Maximum force value
    """
    return df["Force"].max()

def process_csv_files(data_dir: Path, pattern: str = "*.csv") -> List[Dict[str, Any]]:
    """
    Process all CSV files in a directory and extract maximum forces.
    
    Args:
        data_dir: Directory containing CSV files
        pattern: File pattern to match (default: "*.csv")
        
    Returns:
        List of dictionaries containing results for each file
    """
    log = logging.getLogger(__name__)
    
    # Find all CSV files
    csv_files = list(data_dir.glob(pattern))
    
    if not csv_files:
        log.warning(f"No CSV files found in {data_dir}")
        return []
    
    log.info(f"Found {len(csv_files)} CSV files")
    
    results = []
    
    for csv_file in sorted(csv_files):
        try:
            # Load file using standard function
            df = load_file_TT(str(csv_file))
            
            # Extract max force
            max_force = extract_max_force_from_df(df)
            
            # Store results with metadata
            result = {
                "Filename": csv_file.name,
                "Can_Height": df.meta.can_height,
                "Run_Number": df.meta.test_run,
                "Notes": df.meta.notes,
                "Max_Force_N": max_force,
                "Data_Points": len(df),
                "Filepath": str(csv_file)
            }
            
            results.append(result)
            
            notes_display = f"({df.meta.notes})" if df.meta.notes else ""
            log.info(f"{csv_file.name:<35} | {df.meta.can_height:<8} | Run: {df.meta.test_run:<5} | {notes_display:<20} | Max Force: {max_force:>10.4f} N")
            
        except Exception as e:
            log.error(f"Error processing {csv_file.name}: {str(e)}")
    
    log.info(f"Successfully processed {len(results)} files")
    return results

def generate_summary_report(results: List[Dict[str, Any]], output_dir: Path) -> None:
    """
    Generate summary report and statistics from results.
    
    Args:
        results: List of result dictionaries
        output_dir: Directory to save output files
    """
    log = logging.getLogger(__name__)
    
    if not results:
        log.warning("No results to summarize")
        return
    
    # Create results DataFrame
    results_df = pd.DataFrame(results)
    
    # Save detailed results to CSV
    output_file = output_dir / "max_force_summary.csv"
    results_df.to_csv(output_file, index=False)
    log.info(f"Detailed results saved to {output_file}")
    
    # Print overall statistics
    forces = results_df["Max_Force_N"]
    print(f"\n{'='*70}")
    print(f"OVERALL STATISTICS")
    print(f"{'='*70}")
    print(f"Total files processed: {len(results)}")
    print(f"Maximum force: {forces.max():.4f} N")
    print(f"Minimum force: {forces.min():.4f} N")
    print(f"Average force: {forces.mean():.4f} N")
    print(f"Std deviation: {forces.std():.4f} N")
    
    # Group by can height and show statistics
    if "Can_Height" in results_df.columns:
        print(f"\n{'='*70}")
        print(f"STATISTICS BY CAN HEIGHT")
        print(f"{'='*70}")
        height_stats = results_df.groupby("Can_Height")["Max_Force_N"].agg(['count', 'mean', 'std', 'min', 'max'])
        height_stats.columns = ['Count', 'Mean_Force', 'Std_Force', 'Min_Force', 'Max_Force']
        print(height_stats.round(4))
        
        # Save height summary
        height_summary_file = output_dir / "height_summary.csv"
        height_stats.to_csv(height_summary_file)
        log.info(f"Height summary saved to {height_summary_file}")
    
    # Show any special notes found
    notes_present = results_df[results_df["Notes"] != ""]
    if not notes_present.empty:
        print(f"\n{'='*70}")
        print(f"FILES WITH SPECIAL CONDITIONS")
        print(f"{'='*70}")
        for _, row in notes_present.iterrows():
            print(f"  {row['Filename']:<35} | {row['Can_Height']:<8} | Notes: {row['Notes']}")
    
    # Show top 5 highest forces
    print(f"\n{'='*70}")
    print(f"TOP 5 HIGHEST FORCES")
    print(f"{'='*70}")
    top_5 = results_df.nlargest(5, "Max_Force_N")
    for i, (_, row) in enumerate(top_5.iterrows(), 1):
        notes_str = f" ({row['Notes']})" if row['Notes'] else ""
        print(f"  {i}. {row['Filename']:<35} | {row['Can_Height']:<8} | {row['Max_Force_N']:>10.4f} N{notes_str}")

def main() -> None:
    """Main entry point for the CLI tool."""
    parser = argparse.ArgumentParser(
        description="Extract maximum force values from buckling test CSV files",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s -d data/2025-09-11
  %(prog)s -d data/2025-09-11 -o results/ -v
  %(prog)s -d data/2025-09-11 --pattern "*40mm*.csv"
        """
    )
    
    parser.add_argument(
        "-d", "--data-dir",
        type=str,
        required=True,
        help="Directory containing CSV files to process"
    )
    
    parser.add_argument(
        "-o", "--output-dir",
        type=str,
        default=".",
        help="Output directory for results (default: current directory)"
    )
    
    parser.add_argument(
        "--pattern",
        type=str,
        default="*.csv",
        help="File pattern to match (default: *.csv)"
    )
    
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Enable verbose logging"
    )
    
    parser.add_argument(
        "--version",
        action="version",
        version=f"%(prog)s {__version__}"
    )
    
    args = parser.parse_args()
    
    # Set up logging
    log_level = "DEBUG" if args.verbose else "INFO"
    setup_logging(log_level)
    
    log = logging.getLogger(__name__)
    log.info(f"Starting buckling mode experimental analysis v{__version__}")
    
    # Validate input directory
    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        log.error(f"Data directory does not exist: {data_dir}")
        sys.exit(1)
    
    if not data_dir.is_dir():
        log.error(f"Data path is not a directory: {data_dir}")
        sys.exit(1)
    
    # Validate/create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    log.info(f"Processing files in: {data_dir}")
    log.info(f"Output directory: {output_dir}")
    log.info(f"File pattern: {args.pattern}")
    
    # Process CSV files
    results = process_csv_files(data_dir, args.pattern)
    
    if not results:
        log.error("No files were successfully processed")
        sys.exit(1)
    
    # Generate summary report
    generate_summary_report(results, output_dir)
    
    log.info("Analysis complete!")

if __name__ == "__main__":
    main()
